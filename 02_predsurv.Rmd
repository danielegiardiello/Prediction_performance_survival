---
title: "Performance assessment of survival prediction models"
always_allow_html: true
output:
  github_document:
    toc: true
    toc_depth: 5
  keep_text: true
  pandoc_args: --webtex
---
```{r setup, include=FALSE}
# Knitr options
knitr::opts_chunk$set(
  fig.retina = 3,
  fig.path = "imgs/02_predsurv/"
)
```

## Goals
When a risk prediction model has been developed and published in the literature, individual data are probably not available.
In this document, we assume the common scenario that a risk prediction model was already developed and it is available in the literature. We assume that the author(s) developed a risk prediction model using a Cox proportional hazard regression providing the model equation in terms of coefficients and the baseline survival at a fixed time horizon _t_ (e.g. five years).

In summary the goals here are:   
+ Goal 1: to assess the prediction performance of a published risk model with a time-to-event outcome in a new independent (external) data;   
+ Goal 2: to assess the potential clinical utility of a risk prediction model with time-to-event outcome in the new data; 


## Install/load packages and import data
First of all, install the R packages essential for the analyses.
We following libraries are needed to achieve the following goals, if you have not them installed, please use
install.packages('') (e.g. install.packages('survival')) or use the user-friendly approach if you are using RStudio.

```{r, wdlib, message=FALSE,warning=FALSE}

# Use pacman to check whether packages are installed, if not load
if (!require("pacman")) install.packages("pacman")
library(pacman)
pacman::p_load(survival,
               Hmisc,
               pec,
               timeROC)
```


### Data and descriptive statistics
Outcome and predictors in the new data must be coded as provided in the model equation of the developed model.
The time-to-death outcome should be in years and the variables should be categorized exactly as in the developed model.

```{r, create, message=FALSE, warning=FALSE}
# Validation data
gbsg$ryear <- gbsg$rfstime/365.25
gbsg$rfs   <- gbsg$rfs           # the GBSG data contains RFS
gbsg$cnode <- cut(gbsg$nodes, c(-1,0, 3, 51),
                       c("0", "1-3", ">3"))   # categorized node
gbsg$csize <- cut(gbsg$size,  c(-1, 20, 50, 500), #categorized size
                  c("<=20", "20-50", ">50"))
pgr99 <- 1347.85 
gbsg$pgr2 <- pmin(gbsg$pgr, pgr99) # Winsorized value


# Restricted cubic spline for PGR
rcs3_pgr <- rcspline.eval(gbsg$pgr2, knots = c(0, 41, 486))
attr(rcs3_pgr, "dim") <- NULL
attr(rcs3_pgr, "knots") <- NULL
gbsg$pgr3 <- rcs3_pgr


# Much of the analysis will focus on the first 5 years: create
#  data sets that are censored at 5
temp <- survSplit(Surv(ryear, rfs) ~ ., data = gbsg, cut=5,
                  episode ="epoch")
gbsg5 <- subset(temp, epoch==1)

# Relevel
gbsg5$cnode <- relevel(gbsg$cnode, "1-3")
```

### Descriptive statistics of validation data
Descriptive statistics were provided in the table below.
```{r, tab1,message=FALSE, warning=FALSE}
vsel <- gbsg5[, c("pid", "csize", "cnode", "grade", "age", "pgr")]

label(vsel$csize) <- "Size"
label(vsel$cnode) <- "Number of nodes"
label(vsel$grade) <- "Grade of tumor"
label(vsel$age) <- "Age"
label(vsel$pgr) <- "PGR"

units(vsel$csize) <- "mm"
units(vsel$age) <- "years"
units(vsel$pgr) <- "ng/mL"

gtsummary::tbl_summary(
  data = vsel %>% select(-pid),
  label = list(age ~ "Age (years)", csize ~ "Size (cm)", 
               pgr ~ "PGR (ng/mL)",
               grade ~ "Grade"),
  type = all_continuous() ~ "continuous2",
  statistic = all_continuous() ~ c(
    "{mean} ({sd})",
    "{median} ({min}, {max})"
  ),
) %>% 
  gtsummary::as_kable_extra() %>% 
  kableExtra::kable_styling("striped") 

```


We draw the survival and the censoring curves of validation data
```{r, surv,fig.align='center',message=FALSE, warning=FALSE}
# Validation set
sfit_gbsg <- survfit(Surv(ryear, rfs == 1) ~ 1, data = gbsg) # survival
sfit_gbsg_c <- survfit(Surv(ryear, rfs == 0) ~ 1, data = gbsg) # censoring

par(xaxs = "i", yaxs = "i", las = 1)
plot(sfit_gbsg, 
     conf.int = FALSE, 
     lwd = 2, 
     xlab = "Years", 
     bty = "n", 
     xlim = c(0, 8))
lines(sfit_gbsg_c, 
      conf.int = FALSE, 
      col = 2, 
      lwd = 2)
legend("bottomleft", 
       c("Death", "Censoring"), 
       col = 1:2, 
       lwd = 2, 
       bty = "n")
```
A number of 686 patients were selected to externally validate the risk prediction model.The median survival in the validation data was 4 years. The median survival was 4.5 years  while the 5-year survival was 49% (95% CI: 45-54%).

In the prediction model developed using the Rotterdam data, violation of proportional hazards was detected for some predictors and the development data was administratively censored at 5 years. For this reason, we also administratively censor patients in the new (validation) data at 5 years.



## Goal 1: Assessing performance of a developed survival model in a new data
The performance of a risk prediction models may be evaluated through:  

+ discrimination:  the ability of the model to identify patients with and without the outcome and it requires the coefficients (or the log of the hazard ratios) of the developed risk prediction model to be evaluated;     

+ calibration: the agreement between observed and predicted probabilities. It requires the baseline (cumulative) hazard or survival;     
  
+ overall performance measures: as a combination of discrimination and calibration.  
  
Unfortunately, a few publications report the complete baseline (cumulative) hazard or survival or even the baseline (cumulative) hazard or survival at fixed time horizon *t*.  
It is common that physicians focus on one or more clinically relevant time horizons to inform subjects about their risk. 
We aim to assess the prediction performance of a risk prediction model with time-to-event outcome in a new data when information at a fixed time horizon(s) (here at 5 years) of a developed prediction model were provided. The coefficients of the model(s) are essential to assess the overall performances and to calculate the discrimination ability of the developed model in a new data. Information of the baseline hazard/survival over the follow-up time or at a fixed time horizon *t* are needed for calibration assessment.  
When the baseline is not available (and it is not uncommon in the literature), only a graphical representation of the calibration is possible. We assume here to know the coefficients *\beta* and the baseline survival at 5 years *S*<sub>0</sub>*(t = 5)* of the developed prediction model.
We also provide the graphical visualization of the calibration when the baseline is not reported in the literature.

If the model equation is provided including the coefficients and the baseline at fixed time point _t_ (e.g. 5 years), we could validate the risk prediction model in our external data.
Typically, the model equation is provided in terms of predicted survival at a fixed time point _t_.

<img src="https://render.githubusercontent.com/render/math?math=%5Clarge%7B%5Csf%7BS(t)%20%3D%20S_0(t)%5E%7Bexp(%5Cbeta_1X_1%2B%5Cbeta_2X_2%2B%5Cbeta_3X_3%2B%5Ccdots%2B%5Cbeta_pX_p)%7D%7D%3DS_0(t)%5E%7Bexp(PI)%7D%7D">

where: \
*S(t)* is the survival at time *t*. \
*S*<sub>0</sub>*(t)* is the baseline survival at time *t*. \
<img src="https://render.githubusercontent.com/render/math?math=%5Csf%7BPI%20%3D%20%5Cbeta_1X_1%2B%5Ccdots%2B%5Cbeta_pX_p%7D"> is the predictor index: the combination of the coefficients estimated by the statistical model (i.e Cox model) and the predictors. \

In some software, the baseline survival might be already internally rescaled including the centercept. For example, the function ```rms::cph()``` in the ```rms``` R package provides the centercept the formula above included also the centercept in the model equation. More information can be found in 
```help(cph)``` . 
In ```survival``` package the baseline survival can be obtained using
```survival::basehaz()``` then ```exp(-survival::basehaz()$hazard)``` to calculate *S*<sub>0</sub>*(t)* . For details see ```help(basehaz)```, especially the argument ```centered```. 
If the centercept is mentioned in the model equation, this can be used to rescaled the baseline using some easy algebraic steps.

<img src="https://render.githubusercontent.com/render/math?math=%5Clarge%7BS(t)%20%3D%20%7BS_%7B0%7D(t)%7D%5E%7Bexp(PI-c)%7D%20%3D%20%5B%7BS_%7B0%7D(t)%7D%5E%7Bexp(-c)%7D%5D%5E%7Bexp(PI)%7D%20%3D%7BS_%7B0%7D(t)_%7Bresc%7D%7D%5E%7Bexp(PI)%7D%7D">

The model equation assuming the centercept was mentioned in the developed model was:  

<img src="https://render.githubusercontent.com/render/math?math=%5Clarge%7B%5Csf%7BS(5)%20%3D%20.614%5E%7Bexp(.394*I(csize%3D21-50)%2B.623*I(csize%3E50)%2B.361*I(cnode%3D1-3)%2B1.090*I(cnode%3E3)%2B.415*I(cgrade%3E2)-0.920)%7D%7D%7D"> 

The extended model equation including the additional information of the progesterone biomarker was:

<img src="https://render.githubusercontent.com/render/math?math=%5Clarge%7B%5Csf%7BS(5)%20%3D%20.616%5E%7Bexp(.370*I(csize%3D21-50)%2B.598*I(csize%3E50)%2B.385*I(cnode%3D1-3)%2B1.090*I(cnode%3E4)%2B.350*I(cgrade%3E3)-.003*pgr%2B.014*pgr'-0.658)%7D%7D%7D">  


Where the predictors are:   
 - csize is the size of the tumor (categorical variable);  
 - cnode is the number of positive lymph nodes (categorical variable);  
 - grade is the grade of the tumor (categorical variable);  
 - pgr is the progesterone biomarker (continuous variable, used for the extended model);  
 
Pgr was modeled using a three-knot restricted cubic splines to consider the non linear association with the outcome. More details about spline are given in the other html document and in the book 'Regression model strategies' (Harrell, 2nd edition, 2015).
Categorization of number of positive lymph nodes was suboptimal in this example since categorization of continuous predictors is a bad idea.

### 1.1 Calculate the absolute risk prediction at 5 years in the validation data

This part must be run. Then, the user can also focus on one prediction performance is interested in (e.g. discrimination).  

```{r, predsurv, message=FALSE, warning=FALSE}
### Absolute risk  at 5 yrs - Basic model ---------------
# Baseline survival at 5 years - basic model
S0.5yr <- .8604385 
# Design matrix of predictors
des_matr <- as.data.frame(model.matrix(~ csize + cnode + grade, data = gbsg5))
des_matr$`(Intercept)` <- NULL
# Coefficients
coef <- c(0.3922098, 0.6656456, -0.3538533, 0.6936283,  0.3766110)
# Prognostic index (PI)
gbsg5$PI <- as.matrix(des_matr) %*% cbind(coef)
# Absolute risk at 5 years (1 - S(t), 1 - survival at time t)
gbsg5$pred5 <-  1 - S0.5yr**exp(gbsg5$PI)


### Absolute risk  at 5 yrs - Extended model with PGR ---------------
# Baseline survival at 5 years - basic model
S0.5yr_pgr <- .8084657  
# Design matrix of predictors
des_matr <- as.data.frame(model.matrix(~ csize + cnode + grade + 
                                        I(pgr2) + I(pgr3), data = gbsg5))
des_matr$`(Intercept)` <- NULL
# Coefficients
coef <- c(0.37096136, 0.64380754, -0.37445717, 
          0.66988919, 0.32169223, -0.00293231, 0.01281538)
# Prognostic index (PI)
gbsg5$PI_pgr <- as.matrix(des_matr) %*% cbind(coef)
# Absolute risk at 5 years (1 - S(t), 1 - survival at time t)
gbsg5$pred5_pgr <-  1 - S0.5yr_pgr**exp(gbsg5$PI_pgr)
```


### 1.2 Discrimination measures
Discrimination is the ability to differentiate between subjects who have the outcome and subjects who do not.
Concordance can be assessed over several different time intervals:

+ the entire range of the data. Two concordance measures are suggested:    

  + Harrell's C quantifies the degree of concordance as the proportion of such     pairs where the patient with a longer survival time has better predicted      survival;  
  
  + Unoâ€™s C uses a time dependent weighting that more fully adjusts for           censoring;  
  
+ a 5 year window corresponding to our target assessment point. Uno's           time-dependent Area Under the Curve (AUC) is suggested. Uno's time-dependent AUC summarizes discrimination at specific fixed time points. At any time point of interest, _t_, a patient is classified as having an event if the patient experienced the event between baseline and _t_ (5 years in our case study), and as a non-event if the patient remained event-free at _t_. The time-dependent AUC evaluates whether predicted probabilities were higher for cases than for non-case.

Clearly the last of these is most relevant.

This is easy to compute using the concordance function in the survival
package
There is some uncertainty in the literature about the original Harrell
formulation versus Uno's suggestion to re-weight the time scale by the
factor $1/G^2(t)$ where $G$ is the censoring distribution.
There is more detailed information in the concordance vignette found in the
survival package.

We also propose to calculate Uno's time-dependent AUC at a specific time horizon _t_.  
More explanations and details are in the paper.  

The time horizon to calculate the time-dependent measures was set to 5 years.
Values close to 1 indicate good discrimination ability, while values close to 0.5 indicated poor discrimination ability.  

```{r, concordance ,message=FALSE, warning=FALSE}
if (!require("pacman")) install.packages("pacman")
library(pacman)
pacman::p_load(survival,
               Hmisc,
               pec,
               timeROC)

harrell_C_gbsg5 <- concordance(Surv(ryear, rfs) ~ PI, 
                               gbsg5, 
                               reverse = TRUE)

harrell_C_gbsg5_pgr <- concordance(Surv(ryear, rfs) ~ PI_pgr, 
                               gbsg5, 
                               reverse = TRUE)

# Uno's C
Uno_C_gbsg5 <- concordance(Surv(ryear, rfs) ~ PI, 
                           gbsg5, 
                           reverse = TRUE,
                           timewt = "n/G2")

Uno_C_gbsg5_pgr <- concordance(Surv(ryear, rfs) ~ PI_pgr, 
                           gbsg5, 
                           reverse = TRUE,
                           timewt = "n/G2")
```

```{r, concordance_table,warning=FALSE,echo=FALSE}
alpha <- 0.05
temp <- c(
  harrell_C_gbsg5$concordance,
  harrell_C_gbsg5$concordance - 
    qnorm(1 - alpha/2) * sqrt(harrell_C_gbsg5$var),
  harrell_C_gbsg5$concordance + 
    qnorm(1 - alpha/2) * sqrt(harrell_C_gbsg5$var),
  
  harrell_C_gbsg5_pgr$concordance,
  harrell_C_gbsg5_pgr$concordance - 
    qnorm(1 - alpha/2) * sqrt(harrell_C_gbsg5_pgr$var),
  harrell_C_gbsg5$concordance + 
    qnorm(1 - alpha/2) * sqrt(harrell_C_gbsg5_pgr$var),

  Uno_C_gbsg5$concordance,
  Uno_C_gbsg5$concordance - 
    qnorm(1 - alpha/2) * sqrt(Uno_C_gbsg5$var),
  Uno_C_gbsg5$concordance + 
    qnorm(1 - alpha/2) * sqrt(Uno_C_gbsg5$var),
  
  Uno_C_gbsg5_pgr$concordance,
  Uno_C_gbsg5_pgr$concordance - 
    qnorm(1 - alpha/2) * sqrt(Uno_C_gbsg5_pgr$var),
  Uno_C_gbsg5_pgr$concordance + 
    qnorm(1 - alpha/2) * sqrt(Uno_C_gbsg5_pgr$var)
)
res_C  <- matrix(temp, 
                nrow = 2, 
                ncol = 6, 
                byrow = TRUE,
                dimnames = list(
  c("Harrell C - Validation data ", 
    "Uno C - Validation data"),
  
  c(rep(c("Estimate", "Lower .95", "Upper .95"), 2)))
)

res_C <- round(res_C, 2) # Digit
kable(res_C) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 1, "External" = 3, "External + PGR" = 3))
```
Concordance was between 0.64 and 0.68. The extended model slightly improved
discrimination ability compared to the basic model.  


```{r, AUC ,message=FALSE, warning=FALSE}
if (!require("pacman")) install.packages("pacman")
library(pacman)
pacman::p_load(survival,
               Hmisc,
               pec,
               timeROC)

# External validation
Uno_gbsg5 <-
  timeROC(
    T = gbsg5$ryear, delta = gbsg5$rfs,
    marker = gbsg5$PI,
    cause = 1, weighting = "marginal", times = 4.95,
    iid = TRUE
  )

# External validation with pgr
Uno_gbsg5_pgr <-
  timeROC(
    T = gbsg5$ryear, delta = gbsg5$rfs,
    marker = gbsg5$PI_pgr,
    cause = 1, weighting = "marginal", times = 4.95,
    iid = TRUE
  )
# NOTE: if you have a lot of data n > 2000, standard error computation may be really long.
# In that case, please use bootstrap percentile to calculate confidence intervals.
```

```{r, AUC_table,echo=FALSE}
alpha <- .05
k <- 2
res_discr <- matrix(c(
  Uno_gbsg5$AUC["t=4.95"],
  Uno_gbsg5$AUC["t=4.95"] - qnorm(1 - alpha / 2) *
    Uno_gbsg5$inference$vect_sd_1["t=4.95"],
  Uno_gbsg5$AUC["t=4.95"] + qnorm(1 - alpha / 2) *
    Uno_gbsg5$inference$vect_sd_1["t=4.95"],
  Uno_gbsg5_pgr$AUC["t=4.95"],
  Uno_gbsg5_pgr$AUC["t=4.95"] - qnorm(1 - alpha / 2) *
    Uno_gbsg5_pgr$inference$vect_sd_1["t=4.95"],
  Uno_gbsg5_pgr$AUC["t=4.95"] + qnorm(1 - alpha / 2) *
    Uno_gbsg5_pgr$inference$vect_sd_1["t=4.95"]
),
nrow = 1, ncol = 6, byrow = T,
dimnames =
  list(
    c("Uno AUC"),
    rep(c("Estimate", "Lower .95 ", "Upper .95"), 2)
  )
)

res_discr <- round(res_discr, k)
kable(res_discr) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 1, "External" = 3, "External + PGR" = 3))
```

The time-dependent AUCs at 5 years were in the external validation were 0.70 and 0.73 for the basic and extended model, respectively.


### 1.3 Calibration
Calibration is the agreement between observed outcomes and predicted probabilities.
For example, in survival models, a predicted survival probability at a fixed time horizon _t_ of 80% is considered reliable if it can be expected that 80 out of 100 will survive among patients received a predicted survival probability of 80%.
Calibration can be assessed at a fixed time point (e.g. at 5 years), and globally (considering the entire range of the data).
In addition, different level of calibration assessment can be estimated according to the level of information available in the data. 
When individual data of development and validation set are available, full assessment of calibration is possible. Calibration at fixed time point is possible when baseline hazard at fixed time point and coefficient are available. When only coefficients are available, limited assessment of calibration is possible.

In this scenario, we can evaluate calibration only at fixed time point _t_ (i.e. 5 years) since we may have baseline survival at time _t_ (5 years) and coefficients of the model.  

Since different level of information may be available, different level of calibration can be estimated:  mean, weak, and moderate calibration.

+ Mean calibration can be estimated:  

  + at a fixed time point:
    + using the Observed and Expected ratio at time t;  

+ Weak calibration can be estimated:
  + at a fixed time point  
    + using intercept and slope as the coefficient of cloglog                      transformation of predicted probabilities in Cox model. Possible to          determine intercept after adjusting for slope.  

+ Moderate calibration can estimated:
  + at a fixed time point:  
    + using flexible calibration curve, complemented with ICI, E50, E90.  

More detailed explanations are available in the paper.

When a risk prediction model has been developed and published in the literature, individual data are probably not available. 
If the model equation is provided including the coefficients and the baseline at fixed time point _t_ (e.g. 5 years), we could validate the risk prediction model in our external data.
Typically, the model equation is provided in terms of predicted survival at a fixed time point _t_.

#### 1.3.1  Mean calibration - fixed time point
The mean calibration at fixed time point (e.g. at 5 years) can be estimated using the Observed and Expected ratio.
The observed is estimated using the complementary of the Kaplan-Meier curve at the fixed time point.
The expected is estimated using the average predicted risk of the event at the fixed time point.

```{r, mean_cal_t, message=FALSE, warning=FALSE}

##  Observed / Expected ratio at time t ------------
# Observed: 1-Kaplan Meier at time (t)
horizon <- 5
obj <- summary(survfit(Surv(ryear, rfs) ~ 1, 
                       data = gbsg5), 
               times = horizon)

OE <- (1 - obj$surv) / mean(gbsg5$pred5)
OE_pgr <- (1 - obj$surv) / mean(gbsg5$pred5_pgr)
```

```{r, res_OE, message=FALSE, warning=FALSE, echo=FALSE}
alpha <- 0.05
res_OE <- matrix(c(OE,
                   OE * exp(-qnorm(1 - alpha / 2) * sqrt(1 / obj$n.event)),
                   OE * exp(qnorm(1 - alpha / 2) * sqrt(1 / obj$n.event)),
                   
                   OE_pgr,
                   OE_pgr 
                   * exp(-qnorm(1 - alpha / 2) * sqrt(1 / obj$n.event)),
                   OE_pgr 
                   * exp(qnorm(1 - alpha / 2) * sqrt(1 / obj$n.event))
                   ),
                 
                 nrow = 1,
                 ncol = 6,
                 byrow = T,
                 dimnames = list(
                   c("OE ratio"),
                   rep(c("Estimate", "Lower .95", "Upper .95"),2))
)

res_OE <- round(res_OE, 2)
kable(res_OE) %>%
  kable_styling("striped", position = "center") %>%
  add_header_above(c(" " = 1, 
                     "External" = 3, 
                     "External + PGR" = 3))

```
Observed and Expected ratio is 1.07 (95% CI: 0.95 - 1.20) for the basic model and 1.03 (95% CI: 0.92 - 1.16) for the extended model.

#### 1.3.2 Weak calibration - fixed time point
Weak calibration using intercept and slope as the coefficient of cloglog transformation of predicted probabilities in Cox model.  
```{r, weak_cal_t, echo=TRUE,message=FALSE,warning=FALSE}

# cloglog and center for the basic and extended model
lp.val <- log(-log(1 - gbsg5$pred5))   # lp = cloglog
lp.val_pgr <- log(-log(1 - gbsg5$pred5_pgr)) 
center <- mean(lp.val)  # center
center_pgr <- mean(lp.val_pgr)  # center


### Model with a slope and an intercept
f.val <- coxph(Surv(gbsg5$ryear, gbsg5$rfs) ~ lp.val)  
slope <- f.val$coefficients[1]
slope.se <- sqrt(vcov(f.val)[[1, 1]])

f.val_pgr <- coxph(Surv(gbsg5$ryear, gbsg5$rfs) ~ lp.val_pgr)  
slope_pgr <- f.val_pgr$coefficients[1]
slope.se_pgr <- sqrt(vcov(f.val_pgr)[[1, 1]])
 
### same procedure to find the intercept, now with slope-adjusted lp
f.val.offset <- coxph(Surv(gbsg5$ryear, gbsg5$rfs) ~ offset(slope*lp.val))
sf <- survfit(f.val.offset, conf.type = "log-log")
log.H <- log(-log(tail(sf$surv[sf$time <= horizon], 1)))   
int <- log.H - mean(slope*lp.val)
log.H.upper <- log(-log(tail(sf$upper,1)))
int.se <- (log.H-log.H.upper)/qnorm(.975)

# With marker
f.val.offset_pgr <- coxph(Surv(gbsg5$ryear, gbsg5$rfs) ~
                        offset(slope_pgr*lp.val_pgr))
sf_pgr <- survfit(f.val.offset_pgr, conf.type = "log-log")
log.H_pgr <- log(-log(tail(sf_pgr$surv[sf_pgr$time <= horizon], 1)))   
int_pgr <- log.H_pgr - mean(slope_pgr*lp.val_pgr)
log.H.upper_pgr <- log(-log(tail(sf_pgr$upper,1)))
int.se_pgr <- (log.H_pgr-log.H.upper_pgr)/qnorm(.975)

```

```{r, res_cal, message=FALSE, warning=FALSE, echo=FALSE}
alpha <- 0.05
res_cal <- matrix(c(
                int,
                int - qnorm( 1- alpha / 2) * int.se,
                int + qnorm( 1- alpha / 2) * int.se,
                
                
                slope,
                slope - qnorm( 1- alpha / 2) * slope.se,
                slope + qnorm( 1- alpha / 2) * slope.se,
                
                int_pgr,
                int_pgr - qnorm( 1- alpha / 2) * int.se_pgr,
                int_pgr + qnorm( 1- alpha / 2) * int.se_pgr,
                
                slope_pgr,
                slope_pgr - qnorm( 1- alpha / 2) * slope.se_pgr,
                slope_pgr + qnorm( 1- alpha / 2) * slope.se_pgr
                   ),
                 
                 nrow = 2,
                 ncol = 6,
                 byrow = T,
                 dimnames = list(
                   c("Calibration intercept","Calibration slope"),
                   rep(c("Estimate", "Lower .95", "Upper .95"),2))
)

res_cal <- round(res_cal, 2)
kable(res_cal) %>%
  kable_styling("striped", position = "center") %>%
  add_header_above(c(" " = 1, 
                     "External" = 3, 
                     "External + PGR" = 3))
```
Calibration intercept (adjusted for the slope) was 0.15 and 0.14 for the basic and extended model , respectively.  
Calibration slope was 1.08 and 1.17 for the basic and extended model, respectively.

### 1.4 Overall performance measures
Some overall performance measures are proposed using survival data:

+ Brier score: it is the squared differences between observed and predicted values at fixed time point (e.g. at 5 years);
  
+ Scaled Brier score (known as Index of prediction accuracy): it improves interpretability by scaling the Brier Score.

```{r, function_brier, message=FALSE,warning=FALSE, fig.align='center',include=FALSE}
# Run the function to calculate the net benefit and the elements needed to develop decision curve analysis
source(here::here("Functions/brier_score.R"))
```


```{r, overall, warning=FALSE, message=FALSE}
brier_gbsg5 <-
  brier_score(
    tfup = gbsg5$ryear, status = gbsg5$rfs,
    thorizon = 4.95, survival = 1 - gbsg5$pred5
  )

brier_gbsg5b_pgr <-
  brier_score(
    tfup = gbsg5$ryear, status = gbsg5$rfs,
    thorizon = 4.95, survival = 1 - gbsg5$pred5_pgr
  )

## Overall measures: Bootstrap confidence intervals ---------------
B <- 100
horizon <- 4.95
set.seed(12345)
boots_ls <- lapply(seq_len(B), function(b) {
  
  # Resample validation data
  data_boot <- gbsg5[sample(nrow(gbsg5), replace = TRUE), ]

  
  # Get overall measures on boot validation data
  BS_boot <- brier_score(
    tfup = data_boot$ryear, status = data_boot$rfs,
    thorizon = 4.95, survival = 1 - data_boot$pred5
  )
  
  # Get overall measures on boot validation data
  BS_boot_pgr <- brier_score(
    tfup = data_boot$ryear, status = data_boot$rfs,
    thorizon = 4.95, survival = 1 - data_boot$pred5_pgr
  )
    
  brier_boot <- BS_boot["Brier"]
  scaled_brier <- BS_boot["IPA"]
  brier_boot_pgr <- BS_boot_pgr["Brier"]
  scaled_brier_pgr <- BS_boot_pgr["IPA"]
  #.. can add other measure heres, eg. concordance
  
  cbind.data.frame(
    "Brier" = brier_boot,
    "Scaled Brier" = scaled_brier,
    "Brier with PGR" = brier_boot_pgr,
    "Scaled Brier with PGR" = scaled_brier_pgr)
})

df_boots <- do.call(rbind.data.frame, boots_ls)
```

```{r, overall_table, echo=FALSE}
## Table overall measures
alpha <- .05
k <- 2 # number of digits
res_ov <- matrix(c(
  brier_gbsg5["Brier"],
  quantile(df_boots$Brier, probs = alpha / 2),
  quantile(df_boots$Brier, probs = 1 - alpha / 2),
  
  brier_gbsg5b_pgr["Brier"],
  quantile(df_boots$"Brier with PGR", probs = alpha / 2),
  quantile(df_boots$"Brier with PGR", probs = 1 - alpha / 2),
  
  brier_gbsg5["IPA"],
  quantile(df_boots$"Scaled Brier", probs = alpha / 2),
  quantile(df_boots$"Scaled Brier", probs = 1 - alpha / 2),
  brier_gbsg5b_pgr["IPA"],
  quantile(df_boots$"Scaled Brier with PGR", probs = alpha / 2),
  quantile(df_boots$"Scaled Brier with PGR", probs = 1 - alpha / 2)
),
nrow = 2, ncol = 6, byrow = T,
dimnames = list(
  c("Brier", "Scaled Brier"),
  rep(c("Estimate", "Lower .95 ", "Upper .95"), 2)
)
)

res_ov <- round(res_ov, 2) # Digits

kable(res_ov) %>%
  kable_styling("striped") %>%
  add_header_above(c(" " = 1, "External" = 3, "External + PGR" = 3))
```

As expected the overall performance measures were lower in the external validation. Including information about PGR slightly improved the overall performance.


## 2. Clinical utility

Discrimination and calibration measures are essential to assess the
prediction performance but insufficient to evaluate the potential
clinical utility of a risk prediction model for decision making. When
new markers are available, clinical utility assessment evaluates whether
the extended model helps to improve decision making.  
Clinical utility is measured by the net benefit that includes the number
of true positives and the number of false positives. For example, in
time-to-event models, the true positives reflect the benefit of being
event free for a given time horizon using additional interventions such
as additional treatments, personalized follow-up or additional
surgeries. The false positives represent the harms of unnecessary
interventions.  
Generally, in medicine, clinicians accepts to treat a certain number of
patients for which interventions are unnecessary to be event free for a
given time horizon. So, false negatives (the harm of not being event
free for a given time horizon) are more important than false positives
(the harm of unnecessary interventions). Thus, net benefit is the number
of true positives classifications minus the false positives
classifications weighted by a factor related to the harm of not
preventing the event versus unnecessary interventions. The weighting is
derived from the threshold probability to death (one minus survival
probability) using a defined time horizon (for example 5 years since
diagnosis). For example, a threshold of 10% implies that additional
interventions for 10 patients of whom one would have experience the
event in 5 years if untreated is acceptable (thus treating 9 unnecessary
patients). This strategy is compared with the strategies of treat all
and treat none patients. If overtreatment is harmful, a higher threshold
should be used.  
The net benefit is calculated as:

<img src="https://render.githubusercontent.com/render/math?math=%5Chuge%7B%5Cfrac%7BTP%7D%7Bn%7D-%5Cfrac%7BFP%7D%7Bn%7D*%5Cfrac%7Bp_t%7D%7B1-p_t%7D%7D">

*TP*=true positive patients  
*FP*=false positive patients  
*n*=number of patients and *p*<sub>t</sub> is the risk threshold.

For survival data *TP* and *FP* is calculated as follows:  
<img src="https://render.githubusercontent.com/render/math?math=%5CLarge%7BTP%20%3D%20%5B1-S(t)%7C%20X%3D1%5D*P(X%3D1)*n%7D">

<img src="https://render.githubusercontent.com/render/math?math=%5CLarge%7BFP%20%3D%20%5BS(t)%7C%20X%3D1%5D*P(X%3D1)*n%7D">

where  
*S(t)* survival at time *t*  
*X=1* where the predicted probability at time *t* is *p*<sub>t</sub>

And the the decision curve is calculated as follows:

1.  Choose a time horizon (in this case 5 years);
2.  Specify a risk threshold which reflects the ratio between harms and
    benefit of an additional intervention;
3.  Calculate the number of true positive and false positive given the
    threshold specified in (2);
4.  Calculate the net benefit of the survival model;
5.  Plot net benefit on the *y-axis* against the risk threshold on the
    *x-axis*;
6.  Repeat steps 2-4 for each model consideration;
7.  Repeat steps 2-4 for the strategy of assuming all patients are
    treated;
8.  Draw a straight line parallel to the *x-axis* at y=0 representing
    the net benefit associated with the strategy of assuming that all
    patients are not treated.

Given some thresholds, the model/strategy with higher net benefit
represents the one that potentially improves clinical decision making.
However, poor discrimination and calibration lead to lower net benefit.

```{r, function_stdca, message=FALSE,warning=FALSE, fig.align='center',include=FALSE}
# Run the function to calculate the net benefit and the elements needed to develop decision curve analysis
source(here::here("Functions/stdca.R"))
```


```{r, dca, message=FALSE,warning=FALSE}

# Development data
# Predicted probability calculation


# External data
# Validation data
# Predicted probability calculation
gbsg5$mort5_model1 <- 1 - gbsg5$predsurv5

# Extended model with PGR
# Predicted probability calculation
gbsg5$mort5_model1b <- 1 - gbsg5$predsurv5_1b

# Run decision curve analysis

# Validation data
# Model without PGR
gbsg5 <- as.data.frame(gbsg5)
dca_gbsg_model1 <- stdca(
  data = gbsg5, outcome = "rfs", ttoutcome = "ryear",
  timepoint = 5, predictors = "mort5_model1", xstop = 1.0,
  ymin = -0.01, graph = FALSE
)
# Model with PGR
dca_gbsg_model1b <- stdca(
  data = gbsg5, outcome = "rfs", ttoutcome = "ryear",
  timepoint = 5, predictors = "mort5_model1b", xstop = 1,
  ymin = -0.01, graph = FALSE
)

# Decision curves plot
par(xaxs = "i", yaxs = "i", las = 1)
plot(dca_gbsg_model1$net.benefit$threshold,
  dca_gbsg_model1$net.benefit$mort5_model1,
  type = "l", lwd = 2, lty = 1,
  xlab = "Threshold probability in %", ylab = "Net Benefit",
  xlim = c(0, 1), ylim = c(-0.10, 0.60), bty = "n",
  cex.lab = 1.2, cex.axis = 1
)
# legend('topright',c('Treat all','Treat none','Prediction model'),
#        lwd=c(2,2,2),lty=c(1,1,2),col=c('darkgray','black','black'),bty='n')
lines(dca_gbsg_model1$net.benefit$threshold, dca_gbsg_model1$net.benefit$none, type = "l", lwd = 2, lty = 4)
lines(dca_gbsg_model1$net.benefit$threshold, dca_gbsg_model1$net.benefit$all, type = "l", lwd = 2, col = "darkgray")
lines(dca_gbsg_model1b$net.benefit$threshold, dca_gbsg_model1b$net.benefit$mort5_model1b, type = "l", lwd = 2, lty = 5)
legend("topright",
  c(
    "Treat All",
    "Original model",
    "Original model + PGR",
    "Treat None"
  ),
  lty = c(1, 1, 5, 4), lwd = 2, col = c("darkgray", "black", "black", "black"),
  bty = "n"
)
title("B External data", adj = 0, cex = 1.5)
```

Based on previous research we used a range of thresholds from 14% to 23% for adjuvant chemotherapy. If we choose a threshold of 20% the model had a net benefit of 0.385 in the validation data using the basic model. This means that the model would identify 38 patients per 100 who will have recurrent breast cancer or die within 5 years since diagnosis and thus adjuvant chemotherapy is really needed. The decision curve shows that the net benefit compared with intervention in all would be much larger for higher threshold values, i.e., patients accepting higher risks of recurrence. 

Moreover, net benefit can be defined in terms of reduction of avoidable interventions (e.g adjuvant chemotherapy per 100 patients) by:

<img src="https://render.githubusercontent.com/render/math?math=%5Chuge%7B%5Cfrac%7BNB_%7Bmodel%7D%20-%20NB_%7Ball%7D%7D%7B(p_t%2F%20(1-p_t))%7D*100%7D%0A">

where *NB*<sub>model</sub> is the net benefit of the prediction model,
*NB*<sub>all</sub> is the net benefit of the strategy treat all and
*p*<sub>*t*</sub> is the risk threshold.



## References
+ Overall measures   
  Reference: https://diagnprognres.biomedcentral.com/articles/10.1186/s41512-018-0029-2*/ \
  R Vignette: https://cran.r-project.org/web/packages/riskRegression/vignettes/IPA.html#fn.1 \

+ Discrimination measures \ 
  https://www.jstor.org/stable/27639883 \
  https://onlinelibrary.wiley.com/doi/10.1002/sim.5958 \

+ Calibration \
  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3933449/pdf/nihms542648.pdf  \
  https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281   \
  https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8570   \

+ Clinical utility (decision curves)  
  R/SAS/STATA code and references:   
  https://www.mskcc.org/departments/epidemiology-biostatistics/biostatistics/decision-curve-analysis \
  More guidelines about net benefit assessment and interpretation \
  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6261531/ \
  https://diagnprognres.biomedcentral.com/articles/10.1186/s41512-019-0064-7 \
  
+ Other useful references \
  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6728752/ \
  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7100774/ \
  
## Reproducibility ticket

```{r repro_ticket, echo=TRUE}
sessioninfo::session_info()
```
  
